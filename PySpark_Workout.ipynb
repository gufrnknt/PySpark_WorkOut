{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PySpark_Workout",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Installing required packages\n",
        "!pip install pyspark\n",
        "!pip install findspark"
      ],
      "metadata": {
        "id": "7r3y3Jm5SVfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "TwQX5GECBHJq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "WFuu3NgBBHMl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a spark context class\n",
        "sc = SparkContext()\n",
        "\n",
        "# Creating a spark session\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Python Spark DataFrames basic example\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "oYQ1Mqm0BHPP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = range(1,30)\n",
        "# print first element of iterator\n",
        "print(data[0])\n",
        "len(data)\n",
        "xrangeRDD = sc.parallelize(data,4)\n",
        "\n",
        "# this will let us know that we created an RDD\n",
        "xrangeRDD.first"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKDEuKbMBHRu",
        "outputId": "ee0e957f-8d8c-4683-8af1-94ddd580bd54"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method RDD.first of PythonRDD[7] at RDD at PythonRDD.scala:53>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A transformation is an operation on an RDD that results in a new RDD. The transformed RDD is generated rapidly because the new RDD is lazily evaluated, which means that the calculation is not carried out when the new RDD is generated. The RDD will contain a series of transformations, or computation instructions, that will only be carried out when an action is called. In this transformation, we reduce each element in the RDD by 1. Note the use of the lambda function. We also then filter the RDD to only contain elements <10\n"
      ],
      "metadata": {
        "id": "z_v-kVp-Jwmm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subRDD = xrangeRDD.map(lambda x: x-1)\n",
        "filteredRDD = subRDD.filter(lambda x : x<10)\n"
      ],
      "metadata": {
        "id": "waqbwpulBHUa"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(filteredRDD.collect())\n",
        "filteredRDD.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGfXm1_hBHXI",
        "outputId": "ec69fc0e-5e8b-49f9-80e8-a8af0085fe19"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This simple example shows how to create an RDD and cache it. Notice the 10x speed improvement! If you wish to see the actual computation time, browse to the Spark UI...it's at host:4040. You'll see that the second calculation took much less time!"
      ],
      "metadata": {
        "id": "P7JD_GrrKBFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time \n",
        "\n",
        "test = sc.parallelize(range(1,50000),4)\n",
        "test.cache()\n",
        "\n",
        "t1 = time.time()\n",
        "# first count will trigger evaluation of count *and* cache\n",
        "count1 = test.count()\n",
        "dt1 = time.time() - t1\n",
        "print(\"dt1: \", dt1)\n",
        "\n",
        "\n",
        "t2 = time.time()\n",
        "# second count operates on cached data only\n",
        "count2 = test.count()\n",
        "dt2 = time.time() - t2\n",
        "print(\"dt2: \", dt2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35JJ4hWmBHZx",
        "outputId": "7ec35bc2-387e-4a31-cc31-5432705e368e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dt1:  0.755441427230835\n",
            "dt2:  0.3037271499633789\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to work with the extremely powerful SQL engine in Apache Spark, you will need a Spark Session. We have created that in the first Exercise, let us verify that spark session is still active."
      ],
      "metadata": {
        "id": "cRXNFO-qKRDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "id": "eCC9wh2RBHcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the data first into a local `people.json` file\n",
        "!curl https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/people.json >> people.json"
      ],
      "metadata": {
        "id": "BJcYrieLBHef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the dataset into a spark dataframe using the `read.json()` function\n",
        "df = spark.read.json(\"people.json\").cache()"
      ],
      "metadata": {
        "id": "LwJBn92-BHhT"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the dataframe as well as the data schema\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "metadata": {
        "id": "C0Zke3qsSViE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the DataFrame as a SQL temporary view\n",
        "df.createTempView(\"people\")"
      ],
      "metadata": {
        "id": "43oNObq3SVke"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select and show basic data columns\n",
        "t1=time.time()\n",
        "df.select(\"name\").show()\n",
        "t2=time.time()-t1\n",
        "print(t2)\n",
        "t3=time.time()\n",
        "df.select(df[\"name\"]).show()\n",
        "t4=time.time()-t3\n",
        "print(t4)\n",
        "t5=time.time()\n",
        "spark.sql(\"SELECT name FROM people\").show()\n",
        "t6=time.time()-t5\n",
        "print(t6)"
      ],
      "metadata": {
        "id": "8pGo1QAtSVnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform basic filtering\n",
        "\n",
        "df.filter(df[\"age\"] > 21).show()\n",
        "spark.sql(\"SELECT age, name FROM people WHERE age > 21\").show()"
      ],
      "metadata": {
        "id": "gHxi96jESVp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perfom basic aggregation of data\n",
        "\n",
        "df.groupBy(\"age\").count().show()\n",
        "spark.sql(\"SELECT age, COUNT(age) as count FROM people GROUP BY age\").show()"
      ],
      "metadata": {
        "id": "jH9ngfQAKqlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VTDYXwW1KrKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VgwI5qiiKrNK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}