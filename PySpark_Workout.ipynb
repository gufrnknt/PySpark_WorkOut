{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PySpark_Workout",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Installing required packages\n",
        "!pip install pyspark\n",
        "!pip install findspark"
      ],
      "metadata": {
        "id": "7r3y3Jm5SVfP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d7776b4-3854-4b6c-fbec-59849d066368"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 43 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 51.4 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=b447ff0bbf700a3d2a273dd478fae512d01b4f1c0593364c4b22d467d537ae83\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "TwQX5GECBHJq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "WFuu3NgBBHMl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a spark context class\n",
        "sc = SparkContext()\n",
        "\n",
        "# Creating a spark session\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Python Spark DataFrames basic example\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "oYQ1Mqm0BHPP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = range(1,30)\n",
        "# print first element of iterator\n",
        "print(data[0])\n",
        "len(data)\n",
        "xrangeRDD = sc.parallelize(data,4)\n",
        "\n",
        "# this will let us know that we created an RDD\n",
        "xrangeRDD.first"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKDEuKbMBHRu",
        "outputId": "ee0e957f-8d8c-4683-8af1-94ddd580bd54"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method RDD.first of PythonRDD[7] at RDD at PythonRDD.scala:53>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A transformation is an operation on an RDD that results in a new RDD. The transformed RDD is generated rapidly because the new RDD is lazily evaluated, which means that the calculation is not carried out when the new RDD is generated. The RDD will contain a series of transformations, or computation instructions, that will only be carried out when an action is called. In this transformation, we reduce each element in the RDD by 1. Note the use of the lambda function. We also then filter the RDD to only contain elements <10\n"
      ],
      "metadata": {
        "id": "z_v-kVp-Jwmm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subRDD = xrangeRDD.map(lambda x: x-1)\n",
        "filteredRDD = subRDD.filter(lambda x : x<10)\n"
      ],
      "metadata": {
        "id": "waqbwpulBHUa"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(filteredRDD.collect())\n",
        "filteredRDD.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGfXm1_hBHXI",
        "outputId": "ec69fc0e-5e8b-49f9-80e8-a8af0085fe19"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This simple example shows how to create an RDD and cache it. Notice the 10x speed improvement! If you wish to see the actual computation time, browse to the Spark UI...it's at host:4040. You'll see that the second calculation took much less time!"
      ],
      "metadata": {
        "id": "P7JD_GrrKBFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time \n",
        "\n",
        "test = sc.parallelize(range(1,50000),4)\n",
        "test.cache()\n",
        "\n",
        "t1 = time.time()\n",
        "# first count will trigger evaluation of count *and* cache\n",
        "count1 = test.count()\n",
        "dt1 = time.time() - t1\n",
        "print(\"dt1: \", dt1)\n",
        "\n",
        "\n",
        "t2 = time.time()\n",
        "# second count operates on cached data only\n",
        "count2 = test.count()\n",
        "dt2 = time.time() - t2\n",
        "print(\"dt2: \", dt2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35JJ4hWmBHZx",
        "outputId": "7ec35bc2-387e-4a31-cc31-5432705e368e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dt1:  0.755441427230835\n",
            "dt2:  0.3037271499633789\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to work with the extremely powerful SQL engine in Apache Spark, you will need a Spark Session. We have created that in the first Exercise, let us verify that spark session is still active."
      ],
      "metadata": {
        "id": "cRXNFO-qKRDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "eCC9wh2RBHcJ",
        "outputId": "46aa77de-3ab1-4359-df26-ba0d4c336f75"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f0f7db92710>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://ff83a00bad4b:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the data first into a local `people.json` file\n",
        "!curl https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/people.json >> people.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJcYrieLBHef",
        "outputId": "d85b44e1-6d57-4e21-c6c7-3e8b92902985"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100    73  100    73    0     0    133      0 --:--:-- --:--:-- --:--:--   133\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the dataset into a spark dataframe using the `read.json()` function\n",
        "df = spark.read.json(\"people.json\").cache()"
      ],
      "metadata": {
        "id": "LwJBn92-BHhT"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the dataframe as well as the data schema\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "metadata": {
        "id": "C0Zke3qsSViE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b9e5cf9-9879-40a0-90e5-a7481ff24014"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-------+\n",
            "| age|   name|\n",
            "+----+-------+\n",
            "|null|Michael|\n",
            "|  30|   Andy|\n",
            "|  19| Justin|\n",
            "+----+-------+\n",
            "\n",
            "root\n",
            " |-- age: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the DataFrame as a SQL temporary view\n",
        "df.createTempView(\"people\")"
      ],
      "metadata": {
        "id": "43oNObq3SVke"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select and show basic data columns\n",
        "t1=time.time()\n",
        "df.select(\"name\").show()\n",
        "t2=time.time()-t1\n",
        "print(t2)\n",
        "t3=time.time()\n",
        "df.select(df[\"name\"]).show()\n",
        "t4=time.time()-t3\n",
        "print(t4)\n",
        "t5=time.time()\n",
        "spark.sql(\"SELECT name FROM people\").show()\n",
        "t6=time.time()-t5\n",
        "print(t6)"
      ],
      "metadata": {
        "id": "8pGo1QAtSVnO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a19902b9-bf96-479d-f537-2a52682b5ddf"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+\n",
            "|   name|\n",
            "+-------+\n",
            "|Michael|\n",
            "|   Andy|\n",
            "| Justin|\n",
            "+-------+\n",
            "\n",
            "0.2881886959075928\n",
            "+-------+\n",
            "|   name|\n",
            "+-------+\n",
            "|Michael|\n",
            "|   Andy|\n",
            "| Justin|\n",
            "+-------+\n",
            "\n",
            "0.24081754684448242\n",
            "+-------+\n",
            "|   name|\n",
            "+-------+\n",
            "|Michael|\n",
            "|   Andy|\n",
            "| Justin|\n",
            "+-------+\n",
            "\n",
            "0.3472743034362793\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform basic filtering\n",
        "\n",
        "df.filter(df[\"age\"] > 21).show()\n",
        "spark.sql(\"SELECT age, name FROM people WHERE age > 21\").show()"
      ],
      "metadata": {
        "id": "gHxi96jESVp9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ba0e821-6ae2-4539-d539-88886140bc3a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+\n",
            "|age|name|\n",
            "+---+----+\n",
            "| 30|Andy|\n",
            "+---+----+\n",
            "\n",
            "+---+----+\n",
            "|age|name|\n",
            "+---+----+\n",
            "| 30|Andy|\n",
            "+---+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perfom basic aggregation of data\n",
        "\n",
        "df.groupBy(\"age\").count().show()\n",
        "spark.sql(\"SELECT age, COUNT(age) as count FROM people GROUP BY age\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jH9ngfQAKqlo",
        "outputId": "0171e454-fb46-40e4-d8a6-f910919dd65b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+\n",
            "| age|count|\n",
            "+----+-----+\n",
            "|  19|    1|\n",
            "|null|    1|\n",
            "|  30|    1|\n",
            "+----+-----+\n",
            "\n",
            "+----+-----+\n",
            "| age|count|\n",
            "+----+-----+\n",
            "|  19|    1|\n",
            "|null|    0|\n",
            "|  30|    1|\n",
            "+----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VTDYXwW1KrKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VgwI5qiiKrNK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}